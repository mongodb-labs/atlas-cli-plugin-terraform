package hcl

import (
	"fmt"
	"strconv"

	"github.com/hashicorp/hcl/v2"
	"github.com/hashicorp/hcl/v2/hclsyntax"
	"github.com/hashicorp/hcl/v2/hclwrite"
	"github.com/zclconf/go-cty/cty"
)

// ClusterToAdvancedCluster transforms all mongodbatlas_cluster definitions in a
// Terraform configuration file into mongodbatlas_advanced_cluster schema v2 definitions.
// All other resources and data sources are left untouched.
// Note: hclwrite.Tokens are used instead of cty.Value so expressions like var.region can be preserved.
// cty.Value only supports resolved values.
func ClusterToAdvancedCluster(config []byte) ([]byte, error) {
	parser, err := getParser(config)
	if err != nil {
		return nil, err
	}
	for _, resource := range parser.Body().Blocks() {
		labels := resource.Labels()
		resourceName := labels[0]
		if resource.Type() != resourceType || resourceName != cluster {
			continue
		}
		resourceBody := resource.Body()
		labels[0] = advCluster
		resource.SetLabels(labels)

		if resourceBody.FirstMatchingBlock(nRepSpecs, nil) != nil {
			err = fillReplicationSpecs(resourceBody)
		} else {
			err = fillFreeTier(resourceBody)
		}
		if err != nil {
			return nil, err
		}

		resourceBody.AppendNewline()
		appendComment(resourceBody, "Generated by atlas-cli-plugin-terraform.")
		appendComment(resourceBody, "Please confirm that all references to this resource are updated.")
	}
	return parser.Bytes(), nil
}

// fillFreeTier is the entry point to convert clusters in free tier
func fillFreeTier(body *hclwrite.Body) error {
	body.SetAttributeValue(nClusterType, cty.StringVal(valClusterType))
	config := hclwrite.NewEmptyFile()
	configb := config.Body()
	setAttrInt(configb, "priority", valPriority)
	if err := moveAttr(body, configb, nRegionNameSrc, nRegionName, errFreeCluster); err != nil {
		return err
	}
	if err := moveAttr(body, configb, nProviderName, nProviderName, errFreeCluster); err != nil {
		return err
	}
	if err := moveAttr(body, configb, nBackingProviderName, nBackingProviderName, errFreeCluster); err != nil {
		return err
	}
	electableSpec := hclwrite.NewEmptyFile()
	if err := moveAttr(body, electableSpec.Body(), nInstanceSizeSrc, nInstanceSize, errFreeCluster); err != nil {
		return err
	}
	configb.SetAttributeRaw(nElectableSpecs, tokensObject(electableSpec))

	repSpecs := hclwrite.NewEmptyFile()
	repSpecs.Body().SetAttributeRaw(nConfig, tokensArrayObject(config))
	body.SetAttributeRaw(nRepSpecs, tokensArrayObject(repSpecs))
	return nil
}

// fillReplicationSpecs is the entry point to convert clusters with replications_specs (all but free tier)
func fillReplicationSpecs(body *hclwrite.Body) error {
	root, errRoot := extractRootAttrs(body, errRepSpecs)
	if errRoot != nil {
		return errRoot
	}
	repSpecsSrc := body.FirstMatchingBlock(nRepSpecs, nil)
	configSrc := repSpecsSrc.Body().FirstMatchingBlock(nConfigSrc, nil)
	if configSrc == nil {
		return fmt.Errorf("%s: %s not found", errRepSpecs, nConfigSrc)
	}

	body.RemoveAttribute(nNumShards) // num_shards in root is not relevant, only in replication_specs
	// ok moveAttr to fail as cloud_backup is optional
	_ = moveAttr(body, body, nCloudBackup, nBackupEnabled, errRepSpecs)

	repSpecs := hclwrite.NewEmptyFile()
	config, errConfig := getRegionConfigs(configSrc, root)
	if errConfig != nil {
		return errConfig
	}
	repSpecs.Body().SetAttributeRaw(nConfig, config)
	body.SetAttributeRaw(nRepSpecs, tokensArrayObject(repSpecs))

	body.RemoveBlock(repSpecsSrc)
	return nil
}

// extractRootAttrs deletes the attributes common to all replication_specs/regions_config and returns them.
func extractRootAttrs(body *hclwrite.Body, errPrefix string) (attrVals, error) {
	var (
		reqNames = []string{
			nProviderName,
			nInstanceSizeSrc,
		}
		optNames = []string{
			nDiskSizeGB,
			nDiskGBEnabledSrc,
			nComputeEnabledSrc,
			nComputeMinInstanceSizeSrc,
			nComputeMaxInstanceSizeSrc,
			nComputeScaleDownEnabledSrc,
		}
		req = make(map[string]hclwrite.Tokens)
		opt = make(map[string]hclwrite.Tokens)
	)
	for _, name := range reqNames {
		tokens, err := extractAttr(body, name, errPrefix)
		if err != nil {
			return attrVals{}, err
		}
		req[name] = tokens
	}
	for _, name := range optNames {
		tokens, _ := extractAttr(body, name, errPrefix)
		if tokens != nil {
			opt[name] = tokens
		}
	}
	return attrVals{req: req, opt: opt}, nil
}

func getRegionConfigs(configSrc *hclwrite.Block, root attrVals) (hclwrite.Tokens, error) {
	file := hclwrite.NewEmptyFile()
	fileb := file.Body()
	fileb.SetAttributeRaw(nProviderName, root.req[nProviderName])
	if err := moveAttr(configSrc.Body(), fileb, nRegionName, nRegionName, errRepSpecs); err != nil {
		return nil, err
	}
	if err := moveAttr(configSrc.Body(), fileb, nPriority, nPriority, errRepSpecs); err != nil {
		return nil, err
	}
	autoScaling := getAutoScalingOpt(root.opt)
	if autoScaling != nil {
		fileb.SetAttributeRaw(nAutoScaling, autoScaling)
	}
	electableSpecs, errElect := getElectableSpecs(configSrc, root)
	if errElect != nil {
		return nil, errElect
	}
	fileb.SetAttributeRaw(nElectableSpecs, electableSpecs)
	return tokensArrayObject(file), nil
}

func getElectableSpecs(configSrc *hclwrite.Block, root attrVals) (hclwrite.Tokens, error) {
	file := hclwrite.NewEmptyFile()
	fileb := file.Body()
	if err := moveAttr(configSrc.Body(), fileb, nElectableNodes, nNodeCount, errRepSpecs); err != nil {
		return nil, err
	}
	fileb.SetAttributeRaw(nInstanceSize, root.req[nInstanceSizeSrc])
	if root.opt[nDiskSizeGB] != nil {
		fileb.SetAttributeRaw(nDiskSizeGB, root.opt[nDiskSizeGB])
	}
	return tokensObject(file), nil
}

func getAutoScalingOpt(opt map[string]hclwrite.Tokens) hclwrite.Tokens {
	var (
		names = [][2]string{ // use slice instead of map to preserve order
			{nDiskGBEnabledSrc, nDiskGBEnabled},
			{nComputeEnabledSrc, nComputeEnabled},
			{nComputeMinInstanceSizeSrc, nComputeMinInstanceSize},
			{nComputeMaxInstanceSizeSrc, nComputeMaxInstanceSize},
			{nComputeScaleDownEnabledSrc, nComputeScaleDownEnabled},
		}
		file  = hclwrite.NewEmptyFile()
		found = false
	)
	for _, tuple := range names {
		src, dst := tuple[0], tuple[1]
		if tokens := opt[src]; tokens != nil {
			file.Body().SetAttributeRaw(dst, tokens)
			found = true
		}
	}
	if !found {
		return nil
	}
	return tokensObject(file)
}

// moveAttr deletes an attribute from fromBody and adds it to toBody.
func moveAttr(fromBody, toBody *hclwrite.Body, fromAttrName, toAttrName, errPrefix string) error {
	tokens, err := extractAttr(fromBody, fromAttrName, errPrefix)
	if err == nil {
		toBody.SetAttributeRaw(toAttrName, tokens)
	}
	return err
}

// extractAttr deletes an attribute and returns it value.
func extractAttr(body *hclwrite.Body, attrName, errPrefix string) (hclwrite.Tokens, error) {
	attr := body.GetAttribute(attrName)
	if attr == nil {
		return nil, fmt.Errorf("%s: attribute %s not found", errPrefix, attrName)
	}
	tokens := attr.Expr().BuildTokens(nil)
	body.RemoveAttribute(attrName)
	return tokens, nil
}

func setAttrInt(body *hclwrite.Body, attrName string, number int) {
	tokens := hclwrite.Tokens{
		{Type: hclsyntax.TokenNumberLit, Bytes: []byte(strconv.Itoa(number))},
	}
	body.SetAttributeRaw(attrName, tokens)
}

func tokensArrayObject(file *hclwrite.File) hclwrite.Tokens {
	ret := hclwrite.Tokens{
		{Type: hclsyntax.TokenOBrack, Bytes: []byte("[")},
	}
	ret = append(ret, tokensObject(file)...)
	ret = append(ret,
		&hclwrite.Token{Type: hclsyntax.TokenCBrack, Bytes: []byte("]")})
	return ret
}

func tokensObject(file *hclwrite.File) hclwrite.Tokens {
	ret := hclwrite.Tokens{
		{Type: hclsyntax.TokenOBrack, Bytes: []byte("{")},
		{Type: hclsyntax.TokenNewline, Bytes: []byte("\n")},
	}
	ret = append(ret, file.BuildTokens(nil)...)
	ret = append(ret,
		&hclwrite.Token{Type: hclsyntax.TokenCBrack, Bytes: []byte("}")})
	return ret
}

func appendComment(body *hclwrite.Body, comment string) {
	tokens := hclwrite.Tokens{
		&hclwrite.Token{Type: hclsyntax.TokenComment, Bytes: []byte("# " + comment + "\n")},
	}
	body.AppendUnstructuredTokens(tokens)
}

func getParser(config []byte) (*hclwrite.File, error) {
	parser, diags := hclwrite.ParseConfig(config, "", hcl.Pos{Line: 1, Column: 1})
	if diags.HasErrors() {
		return nil, fmt.Errorf("failed to parse Terraform config file: %s", diags.Error())
	}
	return parser, nil
}

type attrVals struct {
	req map[string]hclwrite.Tokens
	opt map[string]hclwrite.Tokens
}

const (
	resourceType = "resource"
	cluster      = "mongodbatlas_cluster"
	advCluster   = "mongodbatlas_advanced_cluster"

	nRepSpecs                   = "replication_specs"
	nConfig                     = "region_configs"
	nConfigSrc                  = "regions_config"
	nElectableSpecs             = "electable_specs"
	nAutoScaling                = "auto_scaling"
	nRegionNameSrc              = "provider_region_name"
	nRegionName                 = "region_name"
	nProviderName               = "provider_name"
	nBackingProviderName        = "backing_provider_name"
	nInstanceSizeSrc            = "provider_instance_size_name"
	nInstanceSize               = "instance_size"
	nClusterType                = "cluster_type"
	nPriority                   = "priority"
	nNumShards                  = "num_shards"
	nBackupEnabled              = "backup_enabled"
	nCloudBackup                = "cloud_backup"
	nDiskSizeGB                 = "disk_size_gb"
	nDiskGBEnabledSrc           = "auto_scaling_disk_gb_enabled"
	nComputeEnabledSrc          = "auto_scaling_compute_enabled"
	nComputeScaleDownEnabledSrc = "auto_scaling_compute_scale_down_enabled"
	nComputeMinInstanceSizeSrc  = "provider_auto_scaling_compute_min_instance_size"
	nComputeMaxInstanceSizeSrc  = "provider_auto_scaling_compute_max_instance_size"
	nDiskGBEnabled              = "disk_gb_enabled"
	nComputeEnabled             = "compute_enabled"
	nComputeScaleDownEnabled    = "compute_scale_down_enabled"
	nComputeMinInstanceSize     = "compute_min_instance_size"
	nComputeMaxInstanceSize     = "compute_max_instance_size"
	nNodeCount                  = "node_count"
	nElectableNodes             = "electable_nodes"

	valClusterType = "REPLICASET"
	valPriority    = 7

	errFreeCluster = "free cluster (because no " + nRepSpecs + ")"
	errRepSpecs    = "setting " + nRepSpecs
)
